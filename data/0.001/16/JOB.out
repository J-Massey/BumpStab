Running SLURM prolog script on ruby004.cluster.local
===============================================================================
Job started on Tue Aug 22 01:47:21 BST 2023
Job ID          : 4636076
Job name        : restart_runs
WorkDir         : /mainfs/scratch/jmom1n15/BumpStab/data/0.001/16
Command         : /mainfs/scratch/jmom1n15/BumpStab/data/0.001/16/subOpt
Partition       : amd
Num hosts       : 16
Num cores       : 1024
Num of tasks    : 1024
Hosts allocated : ruby[004-005,007,014,016,019,021-025,031-032,034-035,040]
Job Output Follows ...
===============================================================================
Starting calculation at Tue Aug 22 01:47:21 BST 2023
---------------------------------------------------------------
make: Entering directory `/mainfs/home/jmom1n15/Lotus/solver/src/geom'
make: `libgeom.a' is up to date.
make: Leaving directory `/mainfs/home/jmom1n15/Lotus/solver/src/geom'
make: Entering directory `/mainfs/home/jmom1n15/Lotus/solver/src/oop'
make: `libfluid.a' is up to date.
make: Leaving directory `/mainfs/home/jmom1n15/Lotus/solver/src/oop'
mpif90 -cpp -DMPION  -Ofast -funroll-loops --param max-unroll-times=4  -I/home/jmom1n15/Lotus/solver/src/oop -I/home/jmom1n15/Lotus/solver/src/geom/geom_lib_include -c lotus.f90
mpif90 -cpp -DMPION  -Ofast -funroll-loops --param max-unroll-times=4  -o lotus lotus.o -L/home/jmom1n15/Lotus/solver/src/oop/. -L/home/jmom1n15/Lotus/solver/src/geom/. -lfluid -lgeom
 Setting up the grid, body and fluid
 -----------------------------------
composite(4096.0)=4096
  products :1*2**12
composite(4096.0)=4096
  products :1*2**12
composite(256.0)=256
  products :1*2**8
 Stretched grid sections:
  negative: r= 0.4%, h_max=21.5
  positive: r= 0.4%, h_max=16.3
 grid symmetry:  F
 Stretched grid sections:
  negative: r= 0.5%, h_max=25.9
  positive: r= 0.5%, h_max=25.9
 grid symmetry:  T
 folder name=/scratch/jmom1n15/BumpStab/data/0.001/16/16/                                    
 Attempting open from: /scratch/jmom1n15/BumpStab/data/0.001/16/16/fluid.vtr.pvd
 Reading from time=   21845.4004     , file=         200
 Starting time update loop
 -----------------------------------
Time:          8.000
Time:          9.000
Time:         10.000
Time:         11.000
 Loop complete: writing restart files and exiting
 -----------------------------------
Number of proccessors :1024
Run folder            :/mainfs/scratch/jmom1n15/BumpStab/data/0.001/16/lotus-data
Read folder           :/scratch/jmom1n15/BumpStab/data/0.001/16/16/
Creating /mainfs/scratch/jmom1n15/BumpStab/data/0.001/16/lotus-data
Setting up in /mainfs/scratch/jmom1n15/BumpStab/data/0.001/16/lotus-data
No postprocessing set up
No stopping criteria, the simulation will run its course
Making executable 
Finished executable 
Running executable 
Run all python files for postprocessing
Popping back up
Reading from: /mainfs/scratch/jmom1n15/BumpStab/data/0.001/16/lotus-data
  0%|          | 0/1606 [00:00<?, ?it/s]  0%|          | 0/1606 [00:00<?, ?it/s]
Traceback (most recent call last):
  File "/mainfs/scratch/jmom1n15/BumpStab/data/watch_simdir.py", line 59, in <module>
    main()
  File "/mainfs/scratch/jmom1n15/BumpStab/data/watch_simdir.py", line 32, in main
    body_snap(directory_to_watch, path, bcount)
  File "/mainfs/scratch/jmom1n15/BumpStab/data/watch_simdir.py", line 18, in body_snap
    bsim.save_sdf_low_memory(fn, count, f"{fnroot}/uvp")
AttributeError: 'ReadIn' object has no attribute 'save_sdf_low_memory'
==============================================================================
Running epilogue script on ruby004.

Submit time  : 2023-08-21T08:26:46
Start time   : 2023-08-22T01:47:20
End time     : 2023-08-23T06:00:34
Elapsed time : 1-04:13:14 (Timelimit=2-12:00:00)

Job ID: 4636076
Cluster: i5
User/Group: jmom1n15/mm
State: FAILED (exit code 1)
Nodes: 16
Cores per node: 64
CPU Utilized: 1198-00:44:23
CPU Efficiency: 99.50% of 1204-01:50:56 core-walltime
Job Wall-clock time: 1-04:13:14
Memory Utilized: 1.47 TB (estimated maximum)
Memory Efficiency: 41.89% of 3.52 TB (3.52 GB/core)

